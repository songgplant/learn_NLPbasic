{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'll try to run this course in my local computer or my google colaboratory...","metadata":{}},{"cell_type":"code","source":"import spacy","metadata":{"execution":{"iopub.status.busy":"2021-12-17T13:27:02.092420Z","iopub.execute_input":"2021-12-17T13:27:02.092839Z","iopub.status.idle":"2021-12-17T13:27:12.328506Z","shell.execute_reply.started":"2021-12-17T13:27:02.092734Z","shell.execute_reply":"2021-12-17T13:27:12.327504Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Loading the spam data (download it from kaggle and upload that file in your directory)\nspam = pd.read_csv('../input/nlp-course/spam.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-17T13:27:51.778183Z","iopub.execute_input":"2021-12-17T13:27:51.778491Z","iopub.status.idle":"2021-12-17T13:27:51.822304Z","shell.execute_reply.started":"2021-12-17T13:27:51.778443Z","shell.execute_reply":"2021-12-17T13:27:51.821672Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# create an empty model\nnlp = spacy.blank(\"en\")\n\n# add the TextCategorizer to the empty model\ntextcat = nlp.add_pipe(\"textcat\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T13:28:49.195175Z","iopub.execute_input":"2021-12-17T13:28:49.195832Z","iopub.status.idle":"2021-12-17T13:28:49.435676Z","shell.execute_reply.started":"2021-12-17T13:28:49.195766Z","shell.execute_reply":"2021-12-17T13:28:49.434839Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Add labels to text classifier\ntextcat.add_label(\"ham\")\ntextcat.add_label(\"spam\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T13:36:17.456120Z","iopub.execute_input":"2021-12-17T13:36:17.456497Z","iopub.status.idle":"2021-12-17T13:36:17.465612Z","shell.execute_reply.started":"2021-12-17T13:36:17.456450Z","shell.execute_reply":"2021-12-17T13:36:17.464755Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_texts = spam['text'].values\ntrain_labels = [{'cats': {'ham': label == 'ham',\n                          'spam': label == 'spam'}} \n                for label in spam['label']]","metadata":{"execution":{"iopub.status.busy":"2021-12-17T13:36:28.127770Z","iopub.execute_input":"2021-12-17T13:36:28.128272Z","iopub.status.idle":"2021-12-17T13:36:28.146595Z","shell.execute_reply.started":"2021-12-17T13:36:28.128239Z","shell.execute_reply":"2021-12-17T13:36:28.145596Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_data = list(zip(train_texts, train_labels))\ntrain_data[:3]","metadata":{"execution":{"iopub.status.busy":"2021-12-17T13:36:36.582392Z","iopub.execute_input":"2021-12-17T13:36:36.582881Z","iopub.status.idle":"2021-12-17T13:36:36.591930Z","shell.execute_reply.started":"2021-12-17T13:36:36.582840Z","shell.execute_reply":"2021-12-17T13:36:36.591307Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from spacy.util import minibatch\nfrom spacy.training.example import Example\n\nspacy.util.fix_random_seed(1)\noptimizer = nlp.begin_training()\n\n# Create the batch generator with batch size = 8\nbatches = minibatch(train_data, size=8)\n# Iterate through minibatches\nfor batch in batches:\n    # Each batch is a list of (text, label) \n    for text, labels in batch:\n        doc = nlp.make_doc(text)\n        example = Example.from_dict(doc, labels)\n        nlp.update([example], sgd=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T13:36:45.704896Z","iopub.execute_input":"2021-12-17T13:36:45.705809Z","iopub.status.idle":"2021-12-17T13:38:11.643440Z","shell.execute_reply.started":"2021-12-17T13:36:45.705765Z","shell.execute_reply":"2021-12-17T13:38:11.642521Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import random\n\nrandom.seed(1)\nspacy.util.fix_random_seed(1)\noptimizer = nlp.begin_training()\n\nlosses = {}\nfor epoch in range(10):\n    random.shuffle(train_data)\n    # Create the batch generator with batch size = 8\n    batches = minibatch(train_data, size=8)\n    # Iterate through minibatches\n    for batch in batches:\n        for text, labels in batch:\n            doc = nlp.make_doc(text)\n            example = Example.from_dict(doc, labels)\n            nlp.update([example], sgd=optimizer, losses=losses)\n    print(losses)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T13:38:11.645318Z","iopub.execute_input":"2021-12-17T13:38:11.645653Z","iopub.status.idle":"2021-12-17T13:52:40.330914Z","shell.execute_reply.started":"2021-12-17T13:38:11.645611Z","shell.execute_reply":"2021-12-17T13:52:40.329888Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"texts = [\"Are you ready for the tea party????? It's gonna be wild\",\n         \"URGENT Reply to this message for GUARANTEED FREE TEA\" ]\ndocs = [nlp.tokenizer(text) for text in texts]\n    \n# Use textcat to get the scores for each doc\ntextcat = nlp.get_pipe('textcat')\nscores = textcat.predict(docs)\n\nprint(scores)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T13:59:54.185917Z","iopub.execute_input":"2021-12-17T13:59:54.186311Z","iopub.status.idle":"2021-12-17T13:59:54.198285Z","shell.execute_reply.started":"2021-12-17T13:59:54.186277Z","shell.execute_reply":"2021-12-17T13:59:54.197490Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# From the scores, find the label with the highest score/probability\npredicted_labels = scores.argmax(axis=1)\nprint([textcat.labels[label] for label in predicted_labels])","metadata":{"execution":{"iopub.status.busy":"2021-12-17T13:59:57.664605Z","iopub.execute_input":"2021-12-17T13:59:57.665110Z","iopub.status.idle":"2021-12-17T13:59:57.669991Z","shell.execute_reply.started":"2021-12-17T13:59:57.665080Z","shell.execute_reply":"2021-12-17T13:59:57.669438Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}